{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating MOF with Target property \n",
    "\n",
    "### 1. Define MOF grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of this cell from https://github.com/AminKaramlou/QNLG\n",
    "\n",
    "from nltk import CFG\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "def _create_mof_search_space():\n",
    "    MOF_GRAMMAR = \"\"\"\n",
    "        M -> T BB\n",
    "        BB -> N E\n",
    "        T -> 'pcu'\n",
    "        N -> 'N106' | 'N123' | 'N139' | 'N144' | 'N248' | 'N394' | 'N155' | 'N173' | 'N205' | 'N505' \n",
    "        E -> 'E14' | 'E70' | 'E220' | 'E15' | 'E8' |'E35' | 'E183' | 'E191' | 'E1' | 'E9' | 'E28' | 'E59' | 'E161' | 'E225' | 'E229' \n",
    "        \"\"\"\n",
    "    \n",
    "    VOCAB = ['pcu', \n",
    "             'N106', 'N123', 'N139', 'N144', 'N248', \n",
    "             'N394', 'N155',  'N173', 'N205', 'N505', \n",
    "             'E14', 'E70', 'E220', 'E15', 'E229',\n",
    "             'E8', 'E35', 'E183', 'E191', 'E1', \n",
    "             'E9', 'E28', 'E59', 'E161', 'E225']\n",
    "    \n",
    "    topology = [\"pcu\"]\n",
    "    building_blocks = ['N106', 'N123', 'N139', 'N144', 'N248', 'N394', 'N155',  'N173', 'N205', 'N505']\n",
    "\n",
    "    GRAMMAR = CFG.fromstring(MOF_GRAMMAR)\n",
    "    MOF = list(generate(GRAMMAR))\n",
    "\n",
    "    def filter_mof(mofs):\n",
    "    # Make sure no MOF appears more than once:\n",
    "        if not len(set(mofs)) == len(mofs):\n",
    "            return False\n",
    "\n",
    "    # Make sure topology appears before node and edge:\n",
    "        try:\n",
    "            topo_position = next(i for i,v in enumerate(mofs) if v in topology)\n",
    "            bb_position = next(i for i,v in enumerate(mofs) if v in building_blocks)\n",
    "        except:\n",
    "            return False\n",
    "        return topo_position < bb_position\n",
    "\n",
    "    MOF = list(filter(filter_mof, MOF))\n",
    "    return MOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate property class of MOFs based on relative probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lambeq import TketModel, spiders_reader, AtomicType, IQPAnsatz\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "model_dir = Path(\"./models/pv\")\n",
    "data_dir = Path(\"./mof_dataset/pv\")\n",
    "\n",
    "backend = AerBackend()\n",
    "backend_config = {\n",
    "    'backend': backend,\n",
    "    'compilation': backend.default_compilation_pass(2),\n",
    "    'shots': 8192\n",
    "}\n",
    "\n",
    "# Map target properties to model names\n",
    "property_to_model = {\n",
    "    \"low\": \"q1\",\n",
    "    \"mod low\": \"q2\",\n",
    "    \"mod high\": \"q3\",\n",
    "    \"high\": \"q4\"\n",
    "}\n",
    "model_names = [\"q1\", \"q2\", \"q3\", \"q4\"]\n",
    "\n",
    "# Load the model by its name\n",
    "def load_model(model_name):\n",
    "    model_path = model_dir / f'{model_name}/qnlp_binary_spider_model_{model_name}.lt'\n",
    "    model = TketModel(backend_config=backend_config)\n",
    "    model.load(str(model_path))\n",
    "    return model\n",
    "\n",
    "# Read true labels from the dataset file\n",
    "def read_true_labels(model_name):\n",
    "    true_labels = {}\n",
    "    file_path = data_dir / f'{model_name}/pv_dataset_{model_name}.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, *mof_name = line.strip().split()\n",
    "            mof_name = \" \".join(mof_name)\n",
    "            true_labels[mof_name] = label\n",
    "    return true_labels\n",
    "\n",
    "# Measure quantum circuit for a given MOF using a specific model\n",
    "def _measure_quantum_circuit_for_mof(model, mof_name):\n",
    "    diagram = spiders_reader.sentences2diagrams([mof_name])\n",
    "    ansatz = IQPAnsatz({AtomicType.NOUN: 0, AtomicType.SENTENCE: 1}, n_layers=1, n_single_qubit_params=3)\n",
    "    circuit = [ansatz(d) for d in diagram]\n",
    "    prediction_probs = model.get_diagram_output(circuit)[0]\n",
    "    return prediction_probs\n",
    "\n",
    "# Output MOF when the randomly generated MOF matches with the target property satisfying confidence threshold\n",
    "def evaluate_until_match(target_property, confidence_threshold=0.85, max_iterations=100):\n",
    "    target_model = property_to_model[target_property]\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        mofs = _create_mof_search_space()\n",
    "        mof_name = \" \".join(random.choice(mofs))\n",
    "        print(f\"Iteration {iteration + 1}: Evaluating MOF: {mof_name} for target property: {target_property}\")\n",
    "\n",
    "        model_predictions = {}\n",
    "\n",
    "        # Load all models and evaluate their predictions\n",
    "        for model_name in model_names:\n",
    "            model = load_model(model_name)\n",
    "            prediction_probs = _measure_quantum_circuit_for_mof(model, mof_name)\n",
    "            model_predictions[model_name] = prediction_probs\n",
    "\n",
    "        total_prob_label_0 = sum(probs[0] for probs in model_predictions.values())\n",
    "        relative_probs = {model: probs[0] / total_prob_label_0 for model, probs in model_predictions.items()}\n",
    "        \n",
    "        # Determine the model with the highest relative probability for label 0\n",
    "        best_model = max(relative_probs, key=relative_probs.get)\n",
    "        best_relative_prob = relative_probs[best_model]\n",
    "\n",
    "        # Map the best-performing model back to the property name\n",
    "        predicted_property = next(key for key, value in property_to_model.items() if value == best_model)\n",
    "\n",
    "        for model_name, probs in model_predictions.items():\n",
    "            print(f\"{model_name}: Prediction Probs: {probs}, Relative Prob Label 0: {relative_probs[model_name]:.3f}\")\n",
    "\n",
    "        # Check if the predicted property matches the target property and relative probability is above threshold\n",
    "        if predicted_property == target_property and best_relative_prob > confidence_threshold:\n",
    "            best_label = 0 if model_predictions[best_model][0] > model_predictions[best_model][1] else 1\n",
    "            true_labels = read_true_labels(best_model)\n",
    "            true_label = true_labels.get(mof_name, \"unknown\")\n",
    "            correctness = \"correct\" if str(best_label) == true_label else \"incorrect\"\n",
    "            print(f\"Best Model: {best_model}, Predicted Property: {predicted_property}, Predicted Label: {best_label}, True Label: {true_label}, Correctness: {correctness}\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    if iteration >= max_iterations:\n",
    "        print(\"Max iterations reached without matching target property\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Evaluating MOF: pcu N106 E229 for target property: high\n",
      "q1: Prediction Probs: [0.048583 0.951417], Relative Prob Label 0: 0.060\n",
      "q2: Prediction Probs: [0. 1.], Relative Prob Label 0: 0.000\n",
      "q3: Prediction Probs: [0.5 0.5], Relative Prob Label 0: 0.619\n",
      "q4: Prediction Probs: [0.25853659 0.74146341], Relative Prob Label 0: 0.320\n",
      "Iteration 2: Evaluating MOF: pcu N123 E8 for target property: high\n",
      "q1: Prediction Probs: [0.14655172 0.85344828], Relative Prob Label 0: 0.144\n",
      "q2: Prediction Probs: [0.59872611 0.40127389], Relative Prob Label 0: 0.586\n",
      "q3: Prediction Probs: [0.2027027 0.7972973], Relative Prob Label 0: 0.198\n",
      "q4: Prediction Probs: [0.07327586 0.92672414], Relative Prob Label 0: 0.072\n",
      "Iteration 3: Evaluating MOF: pcu N505 E225 for target property: high\n",
      "q1: Prediction Probs: [0.13333333 0.86666667], Relative Prob Label 0: 0.201\n",
      "q2: Prediction Probs: [0.32386364 0.67613636], Relative Prob Label 0: 0.489\n",
      "q3: Prediction Probs: [0.02475248 0.97524752], Relative Prob Label 0: 0.037\n",
      "q4: Prediction Probs: [0.18079096 0.81920904], Relative Prob Label 0: 0.273\n",
      "Iteration 4: Evaluating MOF: pcu N394 E229 for target property: high\n",
      "q1: Prediction Probs: [0.01502146 0.98497854], Relative Prob Label 0: 0.019\n",
      "q2: Prediction Probs: [0. 1.], Relative Prob Label 0: 0.000\n",
      "q3: Prediction Probs: [0.60451977 0.39548023], Relative Prob Label 0: 0.753\n",
      "q4: Prediction Probs: [0.18340611 0.81659389], Relative Prob Label 0: 0.228\n",
      "Iteration 5: Evaluating MOF: pcu N106 E191 for target property: high\n",
      "q1: Prediction Probs: [0.23157895 0.76842105], Relative Prob Label 0: 0.193\n",
      "q2: Prediction Probs: [0.10294118 0.89705882], Relative Prob Label 0: 0.086\n",
      "q3: Prediction Probs: [0.83636364 0.16363636], Relative Prob Label 0: 0.696\n",
      "q4: Prediction Probs: [0.0308642 0.9691358], Relative Prob Label 0: 0.026\n",
      "Iteration 6: Evaluating MOF: pcu N205 E8 for target property: high\n",
      "q1: Prediction Probs: [0.10033445 0.89966555], Relative Prob Label 0: 0.123\n",
      "q2: Prediction Probs: [0.2969697 0.7030303], Relative Prob Label 0: 0.363\n",
      "q3: Prediction Probs: [0.40096618 0.59903382], Relative Prob Label 0: 0.490\n",
      "q4: Prediction Probs: [0.02040816 0.97959184], Relative Prob Label 0: 0.025\n",
      "Iteration 7: Evaluating MOF: pcu N106 E14 for target property: high\n",
      "q1: Prediction Probs: [0.10176991 0.89823009], Relative Prob Label 0: 0.088\n",
      "q2: Prediction Probs: [0.99047619 0.00952381], Relative Prob Label 0: 0.860\n",
      "q3: Prediction Probs: [0.03017241 0.96982759], Relative Prob Label 0: 0.026\n",
      "q4: Prediction Probs: [0.02912621 0.97087379], Relative Prob Label 0: 0.025\n",
      "Iteration 8: Evaluating MOF: pcu N144 E191 for target property: high\n",
      "q1: Prediction Probs: [0.02933333 0.97066667], Relative Prob Label 0: 0.022\n",
      "q2: Prediction Probs: [0.32926829 0.67073171], Relative Prob Label 0: 0.246\n",
      "q3: Prediction Probs: [0.93442623 0.06557377], Relative Prob Label 0: 0.699\n",
      "q4: Prediction Probs: [0.04301075 0.95698925], Relative Prob Label 0: 0.032\n",
      "Iteration 9: Evaluating MOF: pcu N144 E35 for target property: high\n",
      "q1: Prediction Probs: [0.01004016 0.98995984], Relative Prob Label 0: 0.010\n",
      "q2: Prediction Probs: [0.03311258 0.96688742], Relative Prob Label 0: 0.033\n",
      "q3: Prediction Probs: [0.80597015 0.19402985], Relative Prob Label 0: 0.795\n",
      "q4: Prediction Probs: [0.16489362 0.83510638], Relative Prob Label 0: 0.163\n",
      "Iteration 10: Evaluating MOF: pcu N106 E15 for target property: high\n",
      "q1: Prediction Probs: [0.71428571 0.28571429], Relative Prob Label 0: 0.738\n",
      "q2: Prediction Probs: [0.12323944 0.87676056], Relative Prob Label 0: 0.127\n",
      "q3: Prediction Probs: [0.04059829 0.95940171], Relative Prob Label 0: 0.042\n",
      "q4: Prediction Probs: [0.08955224 0.91044776], Relative Prob Label 0: 0.093\n",
      "Iteration 11: Evaluating MOF: pcu N106 E15 for target property: high\n",
      "q1: Prediction Probs: [0.81300813 0.18699187], Relative Prob Label 0: 0.760\n",
      "q2: Prediction Probs: [0.15140845 0.84859155], Relative Prob Label 0: 0.142\n",
      "q3: Prediction Probs: [0.02910603 0.97089397], Relative Prob Label 0: 0.027\n",
      "q4: Prediction Probs: [0.07612457 0.92387543], Relative Prob Label 0: 0.071\n",
      "Iteration 12: Evaluating MOF: pcu N173 E35 for target property: high\n",
      "q1: Prediction Probs: [0.18954248 0.81045752], Relative Prob Label 0: 0.229\n",
      "q2: Prediction Probs: [0.00495868 0.99504132], Relative Prob Label 0: 0.006\n",
      "q3: Prediction Probs: [0.5443038 0.4556962], Relative Prob Label 0: 0.659\n",
      "q4: Prediction Probs: [0.08741259 0.91258741], Relative Prob Label 0: 0.106\n",
      "Iteration 13: Evaluating MOF: pcu N248 E35 for target property: high\n",
      "q1: Prediction Probs: [0.04712042 0.95287958], Relative Prob Label 0: 0.075\n",
      "q2: Prediction Probs: [0.0043573 0.9956427], Relative Prob Label 0: 0.007\n",
      "q3: Prediction Probs: [0.47058824 0.52941176], Relative Prob Label 0: 0.748\n",
      "q4: Prediction Probs: [0.10714286 0.89285714], Relative Prob Label 0: 0.170\n",
      "Iteration 14: Evaluating MOF: pcu N155 E161 for target property: high\n",
      "q1: Prediction Probs: [0.03189066 0.96810934], Relative Prob Label 0: 0.032\n",
      "q2: Prediction Probs: [0.00367647 0.99632353], Relative Prob Label 0: 0.004\n",
      "q3: Prediction Probs: [0.0675 0.9325], Relative Prob Label 0: 0.068\n",
      "q4: Prediction Probs: [0.88596491 0.11403509], Relative Prob Label 0: 0.896\n",
      "Best Model: q4, Predicted Property: high, Predicted Label: 0, True Label: 0, Correctness: correct\n"
     ]
    }
   ],
   "source": [
    "evaluate_until_match(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 100 times to evalute generation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_test(target_property, confidence_threshold=0.85, max_iterations=100):\n",
    "    target_model = property_to_model[target_property]\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        mofs = _create_mof_search_space()\n",
    "        mof_name = \" \".join(random.choice(mofs))\n",
    "        \n",
    "        model_predictions = {}\n",
    "\n",
    "        for model_name in model_names:\n",
    "            model = load_model(model_name)\n",
    "            prediction_probs = _measure_quantum_circuit_for_mof(model, mof_name)\n",
    "            model_predictions[model_name] = prediction_probs\n",
    "\n",
    "        total_prob_label_0 = sum(probs[0] for probs in model_predictions.values())\n",
    "        relative_probs = {model: probs[0] / total_prob_label_0 for model, probs in model_predictions.items()}\n",
    "        best_model = max(relative_probs, key=relative_probs.get)\n",
    "        best_relative_prob = relative_probs[best_model]\n",
    "        predicted_property = next(key for key, value in property_to_model.items() if value == best_model)\n",
    "\n",
    "        if predicted_property == target_property and best_relative_prob > confidence_threshold:\n",
    "            true_labels = read_true_labels(best_model)\n",
    "            true_label = true_labels.get(mof_name, \"unknown\")\n",
    "            correctness = \"correct\" if true_labels[mof_name] == \"0\" else \"incorrect\"\n",
    "            return mof_name, model_predictions[best_model], true_label, correctness, iteration + 1\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return mof_name, model_predictions[best_model], \"unknown\", \"timeout\", iteration\n",
    "\n",
    "def evaluate_model_performance(user_property):\n",
    "    correct_guesses = 0\n",
    "    incorrect_guesses = 0\n",
    "    timeouts = 0\n",
    "    total_iterations = 0\n",
    "\n",
    "    for _ in range(100):  \n",
    "        mof_name, prediction_probs, true_label, correctness, iterations = iter_test(user_property)\n",
    "        total_iterations += iterations\n",
    "        \n",
    "        if correctness == \"correct\":\n",
    "            correct_guesses += 1\n",
    "        elif correctness == \"incorrect\":\n",
    "            incorrect_guesses += 1\n",
    "        elif correctness == \"timeout\":\n",
    "            timeouts += 1\n",
    "\n",
    "    average_iterations_per_guess = total_iterations / 100 if total_iterations else 0\n",
    "\n",
    "    print(f\"Total Correct Guesses: {correct_guesses}\")\n",
    "    print(f\"Total Incorrect Guesses: {incorrect_guesses}\")\n",
    "    print(f\"Timeouts: {timeouts}\")\n",
    "    print(f\"Average Iterations per Guess: {average_iterations_per_guess:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Guesses: 90\n",
      "Total Incorrect Guesses: 9\n",
      "Timeouts: 1\n",
      "Average Iterations per Guess: 24.87\n"
     ]
    }
   ],
   "source": [
    "user_property = \"high\"\n",
    "evaluate_model_performance(user_property)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
